{
  
    
        "post0": {
            "title": "Multi-armed Bandits",
            "content": "This post covers Multi-armed bandits, introduces action-value methods and brings in the exploration-exploitation tradeoff 1. . Problem definition - K-armed Bandits . The k-armed Bandit is an environment where at every point - timestep, one must choose an action out of k possible choices - actions. Upon performing the selected action, a reward is returned based on the selection from a stationary probability distribution. . The k-armed Bandit problem is a problem where one must maximize the total expected reward over some time period, i.e. timesteps. Mathematically representing this problem, if at timestep t, action $A_t$ is selected which returns reward $R_t$, then the value of the action, denoted by $q_*(a)$ for an arbitrary action $a$, is the expected or mean reward given the action is selected: . q∗(a)=E[Rt∣At=a]q_*(a) = E[R_t | A_t=a]q∗​(a)=E[Rt​∣At​=a] . Well, you must now be wondering “If I know the exact value of an action, I’m done, right?”. Absolutely! If you know the exact value of an action at every timestep, the solution to the k-armed bandit problem would be a greedy solution i.e. choose the action that returns the maximum reward. But, in most of the environments, this doesn’t hold and that’s why, we’ll have to work with estimates. Now, our aim is to estimate the value of every action($Q_t(a)$) at every timestep to be as close as possible to the actual value($q_*(a)$). Now that we’ve established about the importance of estimates, the actions we take based on these estimates are categorized based on the greedy action i.e action that has the highest estimate at that timestep: . Exploitation : By choosing the greedy action, we are leveraging our current knowledge on all the actions to estimate the best one and going with it. Else …. | Exploration : By choosing the non-greedy action, we are exploring since it improves the estimate of non-greedy actions, thus improving our knowledge for the next time step. | . The trade-off between exploration and exploitation decides the importance between immediate reward and the future rewards. This need of balance is a recurring problem throughout RL. . Action-value methods . Action-value methods, i.e. methods to estimate values of actions abd using these estimates to make decisions. One way of estimating is to average the rewards prior to timestep t, Σi=1t−1Ri.1Ai=aΣi=1t−11Ai=a frac{ Sigma_{i=1}^{t-1} R_i . 1_{A_i=a}}{ Sigma_{i=1}^{t-1} 1_{A_i=a}}Σi=1t−1​1Ai​=a​Σi=1t−1​Ri​.1Ai​=a​​ where $1_{A_i=a}$ is a random variable that’s either 1 or 0 depending on if the action was selected or not. This method is also known as sample-average method. In the case that this action has never previously been taken, a default value is chosen. . Based on our exploitation-exploration strategy, we can either choose to select the best estimate at each time, or choose a different action who’s value estimate isn’t high enough. Choosing exploitation at each step would be equivalent of a greedy algorithm i.e $argmax_{a} Q_t(a)$. To represent exploration mathematically, we introduce a new variable, $ epsilon$ that represents the probability a non-greedy action is chosen at any given timestep irrespective of it’s estimate, emphasis on the independent of the estimate. This non-greedy action is chosen every once in a while. . Computing estimates . While the estimate functions have been established, it’s often expensive to keep track of rewards for every action taken in an environment as it lineary grows as with the timesteps. Therefore, from previous samlpe-average method: . Let Qn=Σi=1n−1Ri/(n−1)Q_n = Sigma_{i=1}^{n-1}R_i / (n-1)Qn​=Σi=1n−1​Ri​/(n−1) . =&gt; Qn+1=Σi=1nRi/(n)Q_{n+1} = Sigma_{i=1}^{n}R_i / (n)Qn+1​=Σi=1n​Ri​/(n) . =&gt; Qn+1=(Qn∗(n−1)+Rn)/(n)Q_{n+1} = (Q_n * (n-1) + R_n) / (n)Qn+1​=(Qn​∗(n−1)+Rn​)/(n) . =&gt; Qn+1=Qn+[Rn−Qn]1nQ_{n+1} = Q_n + [R_n - Q_n] frac{1}{n}Qn+1​=Qn​+[Rn​−Qn​]n1​ . This reduces our memory cost to O(1). But, this is assuming that our rewards probabilities are stationary, i.e. do not change with time. In case of non-stationary problems, let the step size be $ alpha epsilon [0, 1)$. ($ alpha = 1 / n$ in a stationary problem). Now: . Qn+1=Qn+α[Rn−Qn]Q_{n+1} = Q_n + alpha [R_n - Q_n]Qn+1​=Qn​+α[Rn​−Qn​] . =&gt; αRn+(1−α)Qn alpha R_n + (1 - alpha)Q_nαRn​+(1−α)Qn​ . =&gt; αRn+(1−α)[Qn−1+α[Rn−1−Qn−1]] alpha R_n + (1 - alpha) [Q_{n-1} + alpha [R_{n-1} - Q_{n-1}]]αRn​+(1−α)[Qn−1​+α[Rn−1​−Qn−1​]] . Continuting this…., . =&gt; Qn+1=(1−α)nQ1+Σi=1nα∗(1−α)n−1∗RiQ_{n+1} = (1 - alpha)^n Q_1 + Sigma_{i=1}^{n} alpha * (1 - alpha)^{n-1} * R_iQn+1​=(1−α)nQ1​+Σi=1n​α∗(1−α)n−1∗Ri​ . Note that at every timestep, the rewards from the previous timesteps are weighted according to how recent they are i.e. recent rewards are given higher preference to calculate in comparison to the lower estimates by exponentially decreasing $ alpha$. This method is known as recent-weighed average. The caveat here is that this equation doesn’t converge for all values of $ alpha(a)$, as $ alpha(a)$ must conform to : . $ Sigma^{ infty} alpha_n(a) = 1$ | $ Sigma^{ infty} alpha_n^2(a) &lt; infty$ | Bringing out the explorer . Exploration is necessary since the estimates carry uncertainity with them. How do we mathematically encourage this? As mentioned earlier, the first estimate is initialized to a constant since the denominator cannot be zero. But, how does this impact the tradeoff? Let’s say $Q_{0} = +5$, this means the first estimate is 5. Relatively, this is a large number given how future updates can decrease/increase this value. This nudges the algorithm to explore instead of exploit. Similarly, if $Q_{0} = -5$, it is an indication to choose exploitation almost everytime. Thus, setting positive numbers as initial values is termed as Optimisitc Initial Values, a simple trick to encourage exploration. . While epsilon-greedy method does bring exploration to the algorithm, it weighs all estimates equally, irrespective of how close their estimates are to the highest estimate thus neglecting potentially optimal actions. This is rectified in Upper-Confidence Bound Action Selection by modelling uncertaininty in to the greedy equation. . At&lt;=argmaxa[Qt+clntNt(a)]A_t &lt;= argmax_a [Q_t + c sqrt{ frac{ln t}{N_t(a)}}]At​&lt;=argmaxa​[Qt​+cNt​(a)lnt​ . ​] . Here, the uncertainity of a value is represented by the square-root, whose value decreases every time $a$ is chosed i.e as the number of timesteps the action $a$ was selected increases($N_t(a)$), the uncertatinity presumably decreases and the parameter $c$ controls the degree of exploration. . Reference . Reinforcement Learning, Sutton and Baro 2018. &#8617; . |",
            "url": "https://vballoli.github.io/rl-journey/markdown/2020/04/04/bandits.html",
            "relUrl": "/markdown/2020/04/04/bandits.html",
            "date": " • Apr 4, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "Hi! I’m Vaibhav Balloli, final year undergrad at BITS Pilani, Hyderabad Campus. Through this blog, I aim on tracking my journey in Reinforcement Learning by writing blogs, summaries, cheatsheets and Jupyter notebooks to help me and readers understand RL better. I plan on curating simple content using various textbooks, research papers, blogs and youtube channels. I hope you enjoy having posts and letting me ReeL you in! .",
          "url": "https://vballoli.github.io/rl-journey/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

}